{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMY5zSMjcoFl3Sw+U96tUrb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5cdzNc241Vap","executionInfo":{"status":"ok","timestamp":1677853304333,"user_tz":-480,"elapsed":3811,"user":{"displayName":"Meng Wang","userId":"16239936616224806188"}},"outputId":"1dfa6b5f-1a54-44de-9188-8e4f64ac1f5c"},"outputs":[{"output_type":"stream","name":"stdout","text":["1.13.1+cu116\n"]}],"source":["import functools\n","from copy import deepcopy\n","from types import FunctionType\n","from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union\n","\n","import torch\n","import torch.nn as nn\n","print(torch.__version__)\n","from torch._C import ScriptObject  # type: ignore[attr-defined]\n","from torch.fx import Graph, GraphModule, Tracer\n","from torch.fx._symbolic_trace import (_autowrap_check,\n","                                          _patch_wrapped_functions, _Patcher)\n","from torch.fx.proxy import Proxy"]},{"cell_type":"code","source":["_orig_module_call: Callable = nn.Module.__call__\n","_orig_module_getattr: Callable = nn.Module.__getattr__"],"metadata":{"id":"TWMSG3YG1q0l","executionInfo":{"status":"ok","timestamp":1677853314488,"user_tz":-480,"elapsed":726,"user":{"displayName":"Meng Wang","userId":"16239936616224806188"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class UntracedMethodRegistry:\n","    \"\"\"A `Descriptor` class which records untraced methods. Thus, when the\n","    class is traced with CustomTracer, the decorated method will be as a leaf\n","    node, not be nested traced.\n","\n","    Example:\n","        >>> # `imported_cls` is the owner of the untraced method;\n","        >>> # `method_str` is the name of the untraced method.\n","        >>> method_registry = UntracedMethodRegistry(method)\n","        >>> method_registry.__set_name__(imported_cls, method_str)\n","\n","    Args:\n","        method (FunctionType): Function to be registered.\n","    \"\"\"\n","    method_dict: Dict = dict()\n","    tracer = None\n","\n","    def __init__(self, method: FunctionType):\n","        self.method = method\n","        self.owner = None\n","\n","    def __set_name__(self, owner, name):\n","        self.owner = owner\n","        self.name = name\n","        wrapped = self.method_wrapper()\n","        self.method_dict[name] = dict(mod=self.owner, wrapped=wrapped)\n","\n","    def method_wrapper(self):\n","\n","        @functools.wraps(self.method)\n","        def wrapped_method(mod, *args, **kwargs):\n","\n","            def method(*args, **kwargs):\n","                return self.method(mod, *args, **kwargs)\n","\n","            return self.tracer.call_method(mod, self.name, method, args,\n","                                           kwargs)\n","\n","        return wrapped_method"],"metadata":{"id":"rIPKdBWT2Sx0","executionInfo":{"status":"ok","timestamp":1677853317956,"user_tz":-480,"elapsed":2,"user":{"displayName":"Meng Wang","userId":"16239936616224806188"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def _prepare_module_dict(model: torch.nn.Module, fx_graph):\n","    \"\"\"If there is a class method that can not be traced by the symbolic\n","    tracer, a ``call_method`` ``Node`` will be inserted into the ``Graph`` in\n","    ``CustomTracer``.\n","\n","    Example:\n","        >>> class Model:\n","        ...     def __init__(self):\n","        ...         self.head = ClsHead()\n","        ...\n","        >>> class ClsHead(nn.Module):\n","        ...     def forward(self, feats: Tuple[torch.Tensor]) -> torch.Tensor:\n","        ...         return feats[-1]\n","        ...\n","        ...     def loss(self, feats: Tuple[torch.Tensor],\n","        ...              data_samples: List[ClsDataSample], **kwargs) -> dict:\n","        ...         cls_score = self(feats)\n","        ...         # The part can not be traced by torch.fx\n","        ...         losses = self._get_loss(cls_score, data_samples, **kwargs)\n","        ...         return losses\n","        ...\n","        ...     def _get_loss(self, cls_score: torch.Tensor,\n","        ...                   data_samples: List[ClsDataSample], **kwargs):\n","        ...         if 'score' in data_samples[0].gt_label:\n","        ...             xxx\n","        ...         else:\n","        ...             xxx\n","        ...         losses = xxx\n","        ...         return losses\n","\n","    As the ``_get_loss`` can not be traced by torch.fx, ``Toy._get_loss`` need\n","    to be added to ``skipped_methods`` in ``CustomTracer``. Hence the code\n","    above will product the following Graph::\n","\n","    .. code-block:: text\n","        ... ...\n","        %head : [#users=1] = get_attr[target=head]\n","        %_get_loss : [#users=1] = call_method[target=_get_loss](args = (%head, %head_fc, %data_samples), kwargs = {})  # noqa: E501\n","        return _get_loss\n","\n","    Hence, the head module in the ``GraphModule`` and that in the original\n","    model are the same one (refer to https://github.com/pytorch/pytorch/blob/master/torch/fx/graph_module.py#L346).  # noqa: E501\n","    So changes made to the graph module (in ``prepare()``) will also modify\n","    the original model.\n","\n","    Args:\n","        model (torch.nn.Module): Module or function to be\n","            traced and converted into a Graph representation.\n","        fx_graph (torch.fx.Graph): The fx Graph traced by fx tracer. It\n","            contains the nodes this GraphModule should use for code generation.\n","    \"\"\"\n","\n","    def _get_attrs(target, attrs):\n","        attrs = attrs.split('.')\n","        for att in attrs:\n","            target = getattr(target, att)\n","        return target\n","\n","    module_dict = dict()\n","    special_nodes = []\n","\n","    for node in fx_graph.nodes:\n","        if node.op == 'get_attr':\n","            attr = _get_attrs(model, node.target)\n","            if isinstance(attr, nn.Module):\n","                module_dict[node.target] = nn.Module()\n","                special_nodes.append(node)\n","        elif node.op == 'call_method':\n","            for special_node in special_nodes:\n","                if special_node in node.args or \\\n","                        special_node in node.kwargs.values():\n","                    origin_module = getattr(model, special_node.target)\n","                    setattr(module_dict[special_node.target], node.target,\n","                            getattr(origin_module, node.target))\n","\n","    return module_dict"],"metadata":{"id":"CyiDL2jJ2iwy","executionInfo":{"status":"ok","timestamp":1677853321700,"user_tz":-480,"elapsed":400,"user":{"displayName":"Meng Wang","userId":"16239936616224806188"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def duplicate_reused_nodes(graph: Graph, modules: Dict[str, Any] = {}):\n","    \"\"\"Deepcopy the shared modules (e.g. shared detection head in RetinaNet) to\n","    make sure modules can be fused correctly.\n","\n","    Modified from https://github.com/ModelTC/MQBench/blob/main/mqbench/prepare_by_platform.py  # noqa: E501\n","    \"\"\"\n","    _dup_prefix = '_dup'\n","    target_dict = dict()\n","    dup_modules = dict()\n","    for node in graph.nodes:\n","        if node.op == 'call_module':\n","            if node.target not in target_dict:\n","                target_dict[node.target] = [node]\n","            else:\n","                target_dict[node.target].append(node)\n","    for key in target_dict:\n","        if len(target_dict[key]) > 1:\n","            for idx, node in enumerate(target_dict[key]):\n","                if idx == 0:\n","                    continue\n","                module = deepcopy(modules[node.target])\n","                node.target += _dup_prefix + str(idx)\n","                dup_modules[node.target] = module\n","    graph.lint()\n","    return graph, dup_modules"],"metadata":{"id":"cJy8tLBA2qYi","executionInfo":{"status":"ok","timestamp":1677853325123,"user_tz":-480,"elapsed":661,"user":{"displayName":"Meng Wang","userId":"16239936616224806188"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def build_graphmodule(model: torch.nn.Module,\n","                      fx_graph,\n","                      name: str = 'GraphModule'):\n","    \"\"\"To build GraphModule with the generated graph by CustomTracer. The\n","    implement of skipping methods in CustomTracer will cause the confliction of\n","    that a node is both a leaf node and non-leaf node, which will lead that the\n","    modification to the ``graph`` also change the original ``forward``.\n","\n","    Args:\n","        model (torch.nn.Module): Module or function to be\n","            traced and converted into a Graph representation.\n","        fx_graph (torch.fx.Graph): The fx Graph traced by fx tracer. It\n","            contains the nodes this GraphModule should use for code generation.\n","        name (str): The name of generated GraphModule.\n","\n","    Returns:\n","        GraphModule: GraphModule is an nn.Module generated from an fx.Graph.\n","        Graphmodule has a ``graph`` attribute, as well as ``code`` and\n","        ``forward`` attributes generated from that ``graph``.\n","\n","    .. warning::\n","        When ``graph`` is reassigned, ``code`` and ``forward`` will be\n","        automatically regenerated. However, if you edit the contents of the\n","        ``graph`` without reassigning the ``graph`` attribute itself, you must\n","        call ``recompile()`` to update the generated code.\n","    \"\"\"\n","    modules = dict(model.named_modules())\n","    module_dict = _prepare_module_dict(model, fx_graph)\n","    fx_graph, duplicated_modules = duplicate_reused_nodes(fx_graph, modules)\n","    modules.update(module_dict)\n","    modules.update(duplicated_modules)\n","    return GraphModule(modules, fx_graph, name)"],"metadata":{"id":"VXT_l8Dc2rwa","executionInfo":{"status":"ok","timestamp":1677853328019,"user_tz":-480,"elapsed":4,"user":{"displayName":"Meng Wang","userId":"16239936616224806188"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.fx._symbolic_trace import Tracer\n","from torch.fx.node import Target, Node, Argument\n","from torch.nn.intrinsic import _FusedModule\n","from typing import List, Callable, Tuple, Any, Dict, Optional\n","\n","__all__ = [\n","    \"QuantizationTracer\",\n","]\n","\n","class Scope(object):\n","    \"\"\" Scope object that records the module path and the module type\n","    of a module. Scope is used to track the information of the module\n","    that contains a Node in a Graph of GraphModule. For example::\n","        class Sub(torch.nn.Module):\n","            def forward(self, x):\n","                # This will be a call_method Node in GraphModule,\n","                # scope for this would be (module_path=\"sub\", module_type=Sub)\n","                return x.transpose(1, 2)\n","        class M(torch.nn.Module):\n","            def __init__(self):\n","                self.sub = Sub()\n","            def forward(self, x):\n","                # This will be a call_method Node as well,\n","                # scope for this would be (module_path=\"\", None)\n","                x = x.transpose(1, 2)\n","                x = self.sub(x)\n","                return x\n","    \"\"\"\n","\n","    def __init__(self, module_path: str, module_type: Any):\n","        super().__init__()\n","        self.module_path = module_path\n","        self.module_type = module_type\n","\n","\n","class ScopeContextManager(object):\n","    \"\"\" A context manager to track the Scope of Node during symbolic tracing.\n","    When entering a forward function of a Module, we'll update the scope information of\n","    the current module, and when we exit, we'll restore the previous scope information.\n","    \"\"\"\n","\n","    def __init__(\n","        self, scope: Scope, current_module: torch.nn.Module, current_module_path: str\n","    ):\n","        super().__init__()\n","        self.prev_module_type = scope.module_type\n","        self.prev_module_path = scope.module_path\n","        self.scope = scope\n","        self.scope.module_path = current_module_path\n","        self.scope.module_type = type(current_module)\n","\n","    def __enter__(self):\n","        return\n","\n","    def __exit__(self, *args):\n","        self.scope.module_path = self.prev_module_path\n","        self.scope.module_type = self.prev_module_type\n","        return\n","\n","class QuantizationTracer(Tracer):\n","    def __init__(\n","        self, skipped_module_names: List[str], skipped_module_classes: List[Callable]\n","    ):\n","        super().__init__()\n","        self.skipped_module_names = skipped_module_names\n","        self.skipped_module_classes = skipped_module_classes\n","        # NB: initialized the module_type of top level module to None\n","        # we are assuming people won't configure the model with the type of top level\n","        # module here, since people can use \"\" for global config\n","        # We can change this if there is a use case that configures\n","        # qconfig using top level module type\n","        self.scope = Scope(\"\", None)\n","        self.node_name_to_scope: Dict[str, Tuple[str, type]] = {}\n","        self.record_stack_traces = True\n","\n","    def is_leaf_module(self, m: torch.nn.Module, module_qualified_name: str) -> bool:\n","        return (\n","            (\n","                (m.__module__.startswith(\"torch.nn\") or m.__module__.startswith(\"torch.ao.nn\"))\n","                and not isinstance(m, torch.nn.Sequential)\n","            )\n","            or module_qualified_name in self.skipped_module_names\n","            or type(m) in self.skipped_module_classes\n","            or isinstance(m, _FusedModule)\n","        )\n","\n","    def call_module(\n","        self,\n","        m: torch.nn.Module,\n","        forward: Callable[..., Any],\n","        args: Tuple[Any, ...],\n","        kwargs: Dict[str, Any],\n","    ) -> Any:\n","        module_qualified_name = self.path_of_module(m)\n","        # Creating scope with information of current module\n","        # scope will be restored automatically upon exit\n","        with ScopeContextManager(self.scope, m, module_qualified_name):\n","            return super().call_module(m, forward, args, kwargs)\n","\n","    def create_node(\n","        self,\n","        kind: str,\n","        target: Target,\n","        args: Tuple[Argument, ...],\n","        kwargs: Dict[str, Argument],\n","        name: Optional[str] = None,\n","        type_expr: Optional[Any] = None,\n","    ) -> Node:\n","        node = super().create_node(kind, target, args, kwargs, name, type_expr)\n","        self.node_name_to_scope[node.name] = (\n","            self.scope.module_path,\n","            self.scope.module_type,\n","        )\n","        return node"],"metadata":{"id":"InvRD67LVe3Q","executionInfo":{"status":"ok","timestamp":1677853330936,"user_tz":-480,"elapsed":397,"user":{"displayName":"Meng Wang","userId":"16239936616224806188"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def import_modules_from_strings(imports, allow_failed_imports=False):\n","    \"\"\"Import modules from the given list of strings.\n","\n","    Args:\n","        imports (list | str | None): The given module names to be imported.\n","        allow_failed_imports (bool): If True, the failed imports will return\n","            None. Otherwise, an ImportError is raise. Defaults to False.\n","\n","    Returns:\n","        list[module] | module | None: The imported modules.\n","\n","    Examples:\n","        >>> osp, sys = import_modules_from_strings(\n","        ...     ['os.path', 'sys'])\n","        >>> import os.path as osp_\n","        >>> import sys as sys_\n","        >>> assert osp == osp_\n","        >>> assert sys == sys_\n","    \"\"\"\n","    if not imports:\n","        return\n","    single_import = False\n","    if isinstance(imports, str):\n","        single_import = True\n","        imports = [imports]\n","    if not isinstance(imports, list):\n","        raise TypeError(\n","            f'custom_imports must be a list but got type {type(imports)}')\n","    imported = []\n","    for imp in imports:\n","        if not isinstance(imp, str):\n","            raise TypeError(\n","                f'{imp} is of type {type(imp)} and cannot be imported.')\n","        try:\n","            imported_tmp = import_module(imp)\n","        except ImportError:\n","            if allow_failed_imports:\n","                warnings.warn(f'{imp} failed to import and is ignored.',\n","                              UserWarning)\n","                imported_tmp = None\n","            else:\n","                raise ImportError(f'Failed to import {imp}')\n","        imported.append(imported_tmp)\n","    if single_import:\n","        imported = imported[0]\n","    return imported"],"metadata":{"id":"usFtOjj8--NO","executionInfo":{"status":"ok","timestamp":1677853336179,"user_tz":-480,"elapsed":1051,"user":{"displayName":"Meng Wang","userId":"16239936616224806188"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class CustomTracer(QuantizationTracer):\n","    \"\"\"Custom tracer based on QuantizationTracer of pytorch. It can not only\n","    skip some modules and classes while tracing, but also skip some methods\n","    untraced by torch.fx.Tracer.\n","\n","    Args:\n","        skipped_methods (List[str], optional): Methods to be skipped while\n","            tracing. Defaults to None.\n","        skipped_module_names (List[str], optional): Modules to be skipped\n","            while tracing. Defaults to None.\n","        skipped_module_classes (List[Callable], optional): Class to be skipped\n","            while tracing. Defaults to None.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 skipped_methods: List[str] = [],\n","                 skipped_module_names: List[str] = [],\n","                 skipped_module_classes: List[Callable] = [],\n","                 *args,\n","                 **kwargs):\n","        super(CustomTracer, self).__init__(skipped_module_names,\n","                                           skipped_module_classes)\n","        UntracedMethodRegistry.tracer = self  # type: ignore\n","        self.skipped_methods = skipped_methods\n","        if self.skipped_methods:\n","            self.register_skipped_methods()\n","\n","    @staticmethod\n","    def _check_valid_source(source):\n","        \"\"\"Check if the source's format is valid.\"\"\"\n","        if not isinstance(source, str):\n","            raise TypeError(f'source should be a str '\n","                            f'instance, but got {type(source)}')\n","\n","        assert len(source.split('.')) > 1, \\\n","            'source must have at least one `.`'\n","\n","    def register_skipped_methods(self):\n","        \"\"\"Register skipped methods to UntracedMethodRegistry.method_dict.\"\"\"\n","        if not isinstance(self.skipped_methods, list):\n","            self.skipped_methods = [self.skipped_methods]\n","        for s_method in self.skipped_methods:\n","            self._check_valid_source(s_method)\n","            mod_str = '.'.join(s_method.split('.')[:-2])\n","            cls_str = s_method.split('.')[-2]\n","            method_str = s_method.split('.')[-1]\n","\n","            try:\n","                mod = import_modules_from_strings(mod_str)\n","            except ImportError:\n","                raise ImportError(f'{mod_str} is not imported correctly.')\n","\n","            imported_cls: type = getattr(mod, cls_str)\n","            if not isinstance(imported_cls, type):\n","                raise TypeError(f'{cls_str} should be a type '\n","                                f'instance, but got {type(imported_cls)}')\n","            assert hasattr(imported_cls, method_str), \\\n","                   f'{method_str} is not in {mod_str}.'\n","\n","            method = getattr(imported_cls, method_str)\n","\n","            method_registry = UntracedMethodRegistry(method)\n","            method_registry.__set_name__(imported_cls, method_str)\n","\n","    def call_method(self, m: torch.nn.Module, name: str, method: Callable,\n","                    args: Tuple, kwargs: Dict):\n","        \"\"\"Method that specifies the behavior of this ``Tracer`` when it\n","        encounters a call to an ``nn.Module`` instance.\n","\n","        By default, the behavior is to check if the called module is a leaf\n","        module via ``is_leaf_module``. If it is, emit a ``call_module``\n","        node referring to ``m`` in the ``Graph``. Otherwise, call the\n","        ``Module`` normally, tracing through the operations in its ``forward``\n","        function.\n","\n","        This method can be overridden to--for example--create nested traced\n","        GraphModules, or any other behavior you would want while tracing across\n","        ``Module`` boundaries.\n","\n","        Args:\n","            m (torch.nn.Module): The module for which a call is being emitted\n","            name (str): The name of proxy to be created.\n","            method (Callable): The method of the ``Module`` to be invoked\n","            args (Tuple): args of the module callsite\n","            kwargs (Dict): kwargs of the module callsite\n","\n","        Return:\n","\n","            The return value from the Module call. In the case that a\n","            ``call_module`` node was emitted, this is a ``Proxy`` value.\n","            Otherwise, it is whatever value was returned from the ``Module``\n","            invocation.\n","        \"\"\"\n","        # module_qualified_name = self.path_of_module(m)\n","        if not self.is_skipped_method(m):\n","            return method(*args, **kwargs)\n","        args_l = list(args)\n","        args_l.insert(0, m)\n","        args = tuple(args_l)\n","        return self.create_proxy('call_method', name, args, kwargs)\n","\n","    def trace(self,\n","              root: Union[torch.nn.Module, Callable[..., Any]],\n","              concrete_args: Optional[Dict[str, Any]] = None) -> Graph:\n","        \"\"\"Trace ``root`` and return the corresponding FX ``Graph``\n","        representation. ``root`` can either be an ``nn.Module`` instance or a\n","        Python callable. Note that after this call, ``self.root`` may be\n","        different from the ``root`` passed in here. For example, when a free\n","        function is passed to ``trace()``, we will create an ``nn.Module``\n","        instance to use as the root and add embedded constants to.\n","\n","        Args:\n","            root (Union[Module, Callable]): Either a ``Module`` or a function\n","                to be traced through. Backwards-compatibility for this\n","                parameter is guaranteed.\n","            concrete_args (Optional[Dict[str, any]]): Concrete arguments that\n","                should not be treated as Proxies. This parameter is\n","                experimental and its backwards-compatibility is *NOT*\n","                guaranteed.\n","\n","        Returns:\n","            A ``Graph`` representing the semantics of the passed-in ``root``.\n","        \"\"\"\n","        if isinstance(root, torch.nn.Module):\n","            self.root = root\n","            fn = type(root).forward\n","            self.submodule_paths: Optional[Dict[torch.nn.Module, str]] = {\n","                mod: name\n","                for name, mod in root.named_modules()\n","            }\n","        else:\n","            self.root = nn.Module()\n","            fn = root\n","\n","        tracer_cls: Optional[Type['Tracer']] = getattr(self, '__class__', None)\n","        self.graph = Graph(tracer_cls=tracer_cls)\n","\n","        # When we encounter a Tensor value that's not a parameter, we look if\n","        # it is some other attribute on the model. Construct a dict mapping\n","        # Tensor values to the qualified name here for efficiency. This is\n","        # used downstream in create_arg\n","        self.tensor_attrs: Dict[Union[torch.Tensor, ScriptObject], str] = {}\n","\n","        def collect_tensor_attrs(m: nn.Module, prefix_atoms: List[str]):\n","            for k, v in m.__dict__.items():\n","                if isinstance(v, (torch.Tensor, ScriptObject)):\n","                    self.tensor_attrs[v] = '.'.join(prefix_atoms + [k])\n","            for k, v in m.named_children():\n","                collect_tensor_attrs(v, prefix_atoms + [k])\n","\n","        collect_tensor_attrs(self.root, [])\n","\n","        assert isinstance(fn, FunctionType)\n","\n","        fn_globals = fn.__globals__  # run before it gets patched\n","        fn, args = self.create_args_for_root(fn, isinstance(root, nn.Module),\n","                                             concrete_args)\n","\n","        # Reduce number of get_attr calls\n","        parameter_proxy_cache: Dict[str, Proxy] = {}\n","\n","        # Method dispatch on parameters is not recorded unless it's directly\n","        # used. Thus, we need to insert a proxy when __getattr__ requests a\n","        # parameter.\n","        @functools.wraps(_orig_module_getattr)\n","        def module_getattr_wrapper(mod, attr):\n","            attr_val = _orig_module_getattr(mod, attr)\n","            return self.getattr(attr, attr_val, parameter_proxy_cache)\n","\n","        @functools.wraps(_orig_module_call)\n","        def module_call_wrapper(mod, *args, **kwargs):\n","\n","            def forward(*args, **kwargs):\n","                return _orig_module_call(mod, *args, **kwargs)\n","\n","            _autowrap_check(\n","                patcher,\n","                getattr(getattr(mod, 'forward', mod), '__globals__', {}),\n","                self._autowrap_function_ids)\n","            return self.call_module(mod, forward, args, kwargs)\n","\n","        with _Patcher() as patcher:\n","            # allow duplicate patches to support the case of nested calls\n","            patcher.patch_method(\n","                nn.Module,\n","                '__getattr__',\n","                module_getattr_wrapper,\n","                deduplicate=False)\n","            patcher.patch_method(\n","                nn.Module, '__call__', module_call_wrapper, deduplicate=False)\n","\n","            for name, value in UntracedMethodRegistry.method_dict.items():\n","                wrapped = value['wrapped']\n","                patcher.patch_method(\n","                    value['mod'], name, wrapped, deduplicate=False)\n","\n","            _patch_wrapped_functions(patcher)\n","            _autowrap_check(patcher, fn_globals, self._autowrap_function_ids)\n","            for module in self._autowrap_search:\n","                _autowrap_check(patcher, module.__dict__,\n","                                self._autowrap_function_ids)\n","            self.create_node(\n","                'output',\n","                'output', (self.create_arg(fn(*args)), ), {},\n","                type_expr=fn.__annotations__.get('return', None))\n","\n","        self.submodule_paths = None\n","\n","        return self.graph\n","\n","    def is_skipped_method(self, m: torch.nn.Module):\n","        \"\"\"Judge if ``m`` is registered skipped method.\"\"\"\n","        mods = tuple(value['mod']\n","                     for value in UntracedMethodRegistry.method_dict.values())\n","        custom = isinstance(m, mods)\n","        return custom\n","\n","    def is_leaf_module(self, m: torch.nn.Module,\n","                       module_qualified_name: str) -> bool:\n","        \"\"\"A method to specify whether a given ``nn.Module`` is a \"leaf\"\n","        module. Leaf modules are the atomic units that appear in the IR,\n","        referenced by ``call_module`` calls. By default, Modules in the PyTorch\n","        standard library namespace (torch.nn) are leaf modules. All other\n","        modules are traced through and their constituent ops are recorded,\n","        unless specified otherwise via this parameter.\n","\n","        Args:\n","            m (Module): The module being queried about\n","            module_qualified_name (str): The path to root of this module.\n","                For example, if you have a module hierarchy where submodule\n","                ``foo`` contains submodule ``bar``, which contains submodule\n","                ``baz``, that module will appear with the qualified name\n","                ``foo.bar.baz`` here.\n","        \"\"\"\n","        leaf = super().is_leaf_module(m, module_qualified_name)\n","        return leaf"],"metadata":{"id":"Loj57C0s20ZK","executionInfo":{"status":"ok","timestamp":1677853339096,"user_tz":-480,"elapsed":399,"user":{"displayName":"Meng Wang","userId":"16239936616224806188"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def custom_symbolic_trace(\n","        root: Union[torch.nn.Module, Callable[..., Any]],\n","        concrete_args: Optional[Dict[str, Any]] = None) -> GraphModule:\n","    \"\"\"Modified `symbolic_trace` function in pytorch. Given an ``nn.Module`` or\n","    function instance ``root``, this function will return a ``GraphModule``\n","    constructed by recording operations seen while tracing through ``root``.\n","\n","    Args:\n","        root (torch.nn.Module): Module or function to be\n","            traced and converted into a Graph representation.\n","        concrete_args (Optional[Dict[str, any]]): Inputs to be partially\n","            specialized.\n","\n","    Returns:\n","        GraphModule: a Module created from the recorded operations from\n","        ``root``.\n","    \"\"\"\n","    tracer = CustomTracer()\n","    graph = tracer.trace(root, concrete_args)\n","    name = root.__class__.__name__ if isinstance(\n","        root, torch.nn.Module) else root.__name__\n","    return GraphModule(tracer.root, graph, name)\n"],"metadata":{"id":"j_6Pv2zM218a","executionInfo":{"status":"ok","timestamp":1677853344829,"user_tz":-480,"elapsed":471,"user":{"displayName":"Meng Wang","userId":"16239936616224806188"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["!pip install timm==0.8.15.dev0\n","import timm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uy0fQR5e26E5","executionInfo":{"status":"ok","timestamp":1677853363448,"user_tz":-480,"elapsed":7184,"user":{"displayName":"Meng Wang","userId":"16239936616224806188"}},"outputId":"c4a65949-43a4-4ab3-c23e-c1829d1ee9bb"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting timm==0.8.15.dev0\n","  Downloading timm-0.8.15.dev0-py3-none-any.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub\n","  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.8/dist-packages (from timm==0.8.15.dev0) (1.13.1+cu116)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from timm==0.8.15.dev0) (6.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from timm==0.8.15.dev0) (0.14.1+cu116)\n","Collecting safetensors\n","  Downloading safetensors-0.2.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.7->timm==0.8.15.dev0) (4.5.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm==0.8.15.dev0) (2.25.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm==0.8.15.dev0) (3.9.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm==0.8.15.dev0) (23.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm==0.8.15.dev0) (4.64.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->timm==0.8.15.dev0) (8.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->timm==0.8.15.dev0) (1.22.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm==0.8.15.dev0) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm==0.8.15.dev0) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm==0.8.15.dev0) (4.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm==0.8.15.dev0) (1.26.14)\n","Installing collected packages: safetensors, huggingface-hub, timm\n","Successfully installed huggingface-hub-0.12.1 safetensors-0.2.8 timm-0.8.15.dev0\n"]}]},{"cell_type":"code","source":["all_models = timm.list_models('*swinv2*')\n","# print(all_models)\n","from torch.fx import symbolic_trace\n","swin_model = timm.create_model('swin_base_patch4_window7_224', num_classes=1000)\n","swinv2_model = timm.create_model('swinv2_base_window8_256', num_classes=1000)\n","davit_model = timm.create_model('davit_tiny', num_classes=1000)\n","swin_graph_module = custom_symbolic_trace(swin_model)\n","swinv2_graph_module = custom_symbolic_trace(swinv2_model)\n","davit_graph_module = custom_symbolic_trace(davit_model)\n","# swin_graph_module.print_readable()\n","# swin_graph_module.graph.print_tabular()\n","# swinv2_graph_module.print_readable()\n","# swinv2_graph_module.graph.print_tabular()\n","# davit_graph_module.print_readable()\n","# davit_graph_module.graph.print_tabular()\n"],"metadata":{"id":"bn4OagOn3K0T","executionInfo":{"status":"ok","timestamp":1677853384752,"user_tz":-480,"elapsed":17106,"user":{"displayName":"Meng Wang","userId":"16239936616224806188"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"746d78d3-2ac3-4793-8fa9-bfdb254f2f42"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"]}]},{"cell_type":"code","source":["!pip install mmcls==1.0.0rc5 mmcv==2.0.0rc1 mmengine\n","from mmcls.models.backbones.resnet import ResLayer\n","from mmengine.config import Config\n","from mmengine.registry import MODELS\n","cfg = Config.fromfile(\n","    'tests/data/test_models/test_task_modules/mmcls_cfg.py')\n","skipped_methods = [\n","    'mmcls.models.heads.ClsHead._get_loss',\n","    'mmcls.models.heads.ClsHead._get_predictions'\n","]\n","skipped_module_names = ['backbone.layer4.0']\n","skipped_module_classes = [ResLayer]\n","\n","# init without skipped_methods\n","tracer = CustomTracer()\n","assert hasattr(tracer, 'skipped_methods')\n","assert len(tracer.skipped_methods) == 0\n","# init with skipped_methods(list)\n","UntracedMethodRegistry.method_dict = dict()\n","tracer = CustomTracer(skipped_methods=self.skipped_methods)\n","assert '_get_loss' in UntracedMethodRegistry.method_dict.keys()\n","assert '_get_predictions' in UntracedMethodRegistry.method_dict.keys()\n","# init with skipped_methods(str)\n","UntracedMethodRegistry.method_dict = dict()\n","tracer = CustomTracer(skipped_methods=self.skipped_methods[0])\n","assert '_get_loss' in UntracedMethodRegistry.method_dict.keys()\n","\n"," # test trace with skipped_methods\n","model = MODELS.build(cfg.model)\n","UntracedMethodRegistry.method_dict = dict()\n","tracer = CustomTracer(skipped_methods=skipped_methods)\n","graph_tensor = tracer.trace(model, concrete_args={'mode': 'tensor'})\n","graph_loss = tracer.trace(model, concrete_args={'mode': 'loss'})\n","graph_predict = tracer.trace(model, concrete_args={'mode': 'predict'})\n","assert isinstance(graph_tensor, Graph)\n","assert isinstance(graph_loss, Graph)\n","skip_flag_loss = False\n","for node in graph_loss.nodes:\n","    if node.op == 'call_method' and node.target == '_get_loss':\n","        skip_flag_loss = True\n","assert isinstance(graph_predict, Graph)\n","skip_flag_predict = False\n","for node in graph_predict.nodes:\n","    if node.op == 'call_method' and node.target == '_get_predictions':\n","        skip_flag_predict = True\n","assert skip_flag_loss and skip_flag_predict\n","\n","# test trace with skipped_module_names\n","model = MODELS.build(cfg.model)\n","UntracedMethodRegistry.method_dict = dict()\n","tracer = CustomTracer(skipped_module_names=skipped_module_names)\n","graph_tensor = tracer.trace(model, concrete_args={'mode': 'tensor'})\n","skip_flag = False\n","for node in graph_tensor.nodes:\n","    skipped_module_name = skipped_module_names[0]\n","    if node.op == 'call_module' and node.target == skipped_module_name:\n","        skip_flag = True\n","assert skip_flag"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"tC27Ner72_PC","executionInfo":{"status":"error","timestamp":1677855253777,"user_tz":-480,"elapsed":1052529,"user":{"displayName":"Meng Wang","userId":"16239936616224806188"}},"outputId":"b9c19351-0c90-492e-e1d8-da8f59948ea8"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: mmcls==1.0.0rc5 in /usr/local/lib/python3.8/dist-packages (1.0.0rc5)\n","Collecting mmcv==2.0.0rc1\n","  Using cached mmcv-2.0.0rc1.tar.gz (406 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: mmengine in /usr/local/lib/python3.8/dist-packages (0.6.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from mmcls==1.0.0rc5) (23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from mmcls==1.0.0rc5) (1.22.4)\n","Requirement already satisfied: modelindex in /usr/local/lib/python3.8/dist-packages (from mmcls==1.0.0rc5) (0.0.2)\n","Requirement already satisfied: rich in /usr/local/lib/python3.8/dist-packages (from mmcls==1.0.0rc5) (13.3.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from mmcls==1.0.0rc5) (3.5.3)\n","Requirement already satisfied: addict in /usr/local/lib/python3.8/dist-packages (from mmcv==2.0.0rc1) (2.4.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from mmcv==2.0.0rc1) (8.4.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from mmcv==2.0.0rc1) (6.0)\n","Requirement already satisfied: yapf in /usr/local/lib/python3.8/dist-packages (from mmcv==2.0.0rc1) (0.32.0)\n","Requirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.8/dist-packages (from mmengine) (4.6.0.66)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from mmengine) (2.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mmcls==1.0.0rc5) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mmcls==1.0.0rc5) (2.8.2)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mmcls==1.0.0rc5) (4.38.0)\n","Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mmcls==1.0.0rc5) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mmcls==1.0.0rc5) (1.4.4)\n","Requirement already satisfied: model-index in /usr/local/lib/python3.8/dist-packages (from modelindex->mmcls==1.0.0rc5) (0.1.11)\n","Requirement already satisfied: markdown-it-py<3.0.0,>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from rich->mmcls==1.0.0rc5) (2.2.0)\n","Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from rich->mmcls==1.0.0rc5) (4.5.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.14.0 in /usr/local/lib/python3.8/dist-packages (from rich->mmcls==1.0.0rc5) (2.14.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.8/dist-packages (from markdown-it-py<3.0.0,>=2.1.0->rich->mmcls==1.0.0rc5) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib->mmcls==1.0.0rc5) (1.15.0)\n","Requirement already satisfied: ordered-set in /usr/local/lib/python3.8/dist-packages (from model-index->modelindex->mmcls==1.0.0rc5) (4.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from model-index->modelindex->mmcls==1.0.0rc5) (8.1.3)\n","Requirement already satisfied: markdown in /usr/local/lib/python3.8/dist-packages (from model-index->modelindex->mmcls==1.0.0rc5) (3.4.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown->model-index->modelindex->mmcls==1.0.0rc5) (6.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown->model-index->modelindex->mmcls==1.0.0rc5) (3.15.0)\n","Building wheels for collected packages: mmcv\n","  Building wheel for mmcv (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mmcv: filename=mmcv-2.0.0rc1-cp38-cp38-linux_x86_64.whl size=28115159 sha256=d32719d080c9861260c6c98af489fe6981401f88ff79e1ee8f525999442bbe44\n","  Stored in directory: /root/.cache/pip/wheels/5d/d3/64/54e29987d1fb1abb6ea08307121a60d6a84463e43603956fc2\n","Successfully built mmcv\n","Installing collected packages: mmcv\n","  Attempting uninstall: mmcv\n","    Found existing installation: mmcv 1.7.1\n","    Uninstalling mmcv-1.7.1:\n","      Successfully uninstalled mmcv-1.7.1\n","Successfully installed mmcv-2.0.0rc1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["mmcv"]}}},"metadata":{}},{"output_type":"error","ename":"AssertionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-5f88bb0e2133>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install mmcls==1.0.0rc5 mmcv==2.0.0rc1 mmengine'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmmcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbones\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmmengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmmengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMODELS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m cfg = Config.fromfile(\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/mmcls/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmmengine_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdigit_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmmengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m assert (mmcv_version >= digit_version(mmcv_minimum_version)\n\u001b[0m\u001b[1;32m     18\u001b[0m         and mmcv_version < digit_version(mmcv_maximum_version)), \\\n\u001b[1;32m     19\u001b[0m     \u001b[0;34mf'MMCV=={mmcv.__version__} is used but incompatible. '\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: MMCV==1.7.1 is used but incompatible. Please install mmcv>=2.0.0rc1, <2.0.0."]}]}]}