{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgHsq8xSguc2"
      },
      "source": [
        "See the AOTInductor tutorial full example at https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_aot_inductor.rst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7ZAT58zi2m9",
        "outputId": "640bfce8-6737-4803-efda-e6ed130d6490"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 2.4.0.dev20240426+cu121\n",
            "Uninstalling torch-2.4.0.dev20240426+cu121:\n",
            "  Successfully uninstalled torch-2.4.0.dev20240426+cu121\n",
            "Looking in indexes: https://download.pytorch.org/whl/nightly/cu121\n",
            "Collecting torch\n",
            "  Using cached https://download.pytorch.org/whl/nightly/cu121/torch-2.4.0.dev20240426%2Bcu121-cp310-cp310-linux_x86_64.whl (795.6 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: pytorch-triton==3.0.0+45fff310c8 in /usr/local/lib/python3.10/dist-packages (from torch) (3.0.0+45fff310c8)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.1.105)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: torch\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.14 requires torch<2.3,>=1.10, but you have torch 2.4.0.dev20240426+cu121 which is incompatible.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.4.0.dev20240426+cu121 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.4.0.dev20240426+cu121 which is incompatible.\n",
            "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 2.4.0.dev20240426+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.4.0.dev20240426+cu121\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Install the latest PyTorch Build.\n",
        "ETA: 1 minute\n",
        "\"\"\"\n",
        "# resolve dependency conflict on colab and may not be necessary on local environement\n",
        "\n",
        "!pip uninstall torch -y\n",
        "\n",
        "!pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4kjLad0egzfP",
        "outputId": "a5df59d3-de49-49d6-9a9d-d427628404e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/model.so\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "More details on the torch._export.aot_compile API at https://github.com/pytorch/pytorch/blob/cd06c73cbd398811efc4afe85ee29dee64ebfd45/torch/_export/__init__.py#L320\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = torch.nn.Linear(10, 16)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.fc2 = torch.nn.Linear(16, 1)\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "with torch.no_grad():\n",
        "    model = Model().to(device=device)\n",
        "    example_inputs=(torch.randn(8, 10, device=device),)\n",
        "    batch_dim = torch.export.Dim(\"batch\", min=1, max=1024)\n",
        "    so_path = torch._export.aot_compile(\n",
        "        model,\n",
        "        example_inputs,\n",
        "        # Specify the first dimension of the input x as dynamic\n",
        "        dynamic_shapes={\"x\": {0: batch_dim}},\n",
        "        # Specify the generated shared library path\n",
        "        options={\"aot_inductor.output_path\": os.path.join(os.getcwd(), \"model.so\")},\n",
        "    )\n",
        "    print(so_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM2gpcoy2Ou3"
      },
      "source": [
        "https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_aot_inductor.rst contains an example on how to run the following C++ inference example by building with cmake. For this demo, we will not compile and run the following C++ code. We will use a pybind-ed runner to load the generated model.so back to Python.\n",
        "\n",
        "```\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "\n",
        "#include <torch/torch.h>\n",
        "#include <torch/csrc/inductor/aoti_runner/model_container_runner_cuda.h>\n",
        "\n",
        "int main() {\n",
        "    c10::InferenceMode mode;\n",
        "\n",
        "    torch::inductor::AOTIModelContainerRunnerCuda runner(\"model.so\");\n",
        "    std::vector<torch::Tensor> inputs = {torch::randn({8, 10}, at::kCUDA)};\n",
        "    std::vector<torch::Tensor> outputs = runner.run(inputs);\n",
        "    std::cout << \"Result from the first inference:\"<< std::endl;\n",
        "    std::cout << outputs[0] << std::endl;\n",
        "\n",
        "    // The second inference uses a different batch size and it works because we\n",
        "    // specified that dimension as dynamic when compiling model.so.\n",
        "    std::cout << \"Result from the second inference:\"<< std::endl;\n",
        "    std::vector<torch::Tensor> inputs2 = {torch::randn({2, 10}, at::kCUDA)};\n",
        "    std::cout << runner.run(inputs2)[0] << std::endl;\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TgSxNP4Swe4A",
        "outputId": "e90a26c7-4644-4d0f-8d8b-e3e07a257fa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.6255],\n",
            "        [0.5078],\n",
            "        [0.5407],\n",
            "        [0.5807],\n",
            "        [0.5801],\n",
            "        [0.6062],\n",
            "        [0.5439],\n",
            "        [0.6316]])\n"
          ]
        }
      ],
      "source": [
        "# Use python runner utility to load the generated model.so\n",
        "aot_compiled = torch._export.aot_load(os.path.join(os.getcwd(), \"model.so\"), device=device)\n",
        "\n",
        "input1 = torch.randn(8, 10, device=device)\n",
        "print(aot_compiled(input1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OoYdX9fiuWh7",
        "outputId": "cc033b91-fcf1-4106-951b-25d36a77204b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.5668],\n",
            "        [0.6103],\n",
            "        [0.5271],\n",
            "        [0.5384],\n",
            "        [0.5847],\n",
            "        [0.5622],\n",
            "        [0.5398],\n",
            "        [0.5522],\n",
            "        [0.4717],\n",
            "        [0.5272],\n",
            "        [0.5258],\n",
            "        [0.5603],\n",
            "        [0.5213],\n",
            "        [0.3987],\n",
            "        [0.4904],\n",
            "        [0.6111],\n",
            "        [0.5210],\n",
            "        [0.4858],\n",
            "        [0.5139],\n",
            "        [0.5684]])\n"
          ]
        }
      ],
      "source": [
        "# Because the model was compiled with a dynamic batch size, we can run prediction  a different batch size\n",
        "input2 = torch.randn(20, 10, device=device)\n",
        "print(aot_compiled(input2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pS0tlwTKNROW",
        "outputId": "f6ac2e6d-c9de-412f-a1ef-75396331fec5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] Output code: \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] #include <torch/csrc/inductor/aoti_runtime/arrayref_tensor.h>\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] #include <torch/csrc/inductor/aoti_runtime/interface.h>\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] #include <torch/csrc/inductor/aoti_runtime/model_container.h>\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] #include <torch/csrc/inductor/aoti_runtime/scalar_to_tensor.h>\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] #include <torch/csrc/inductor/aoti_runtime/thread_local.h>\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] #include <iostream>\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] #include <sstream>\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] #include <stdexcept>\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] #include <vector>\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] #define CONVERT_EXCEPTION_TO_ERROR_CODE(...)                 \\\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   try {                                                      \\\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     __VA_ARGS__                                              \\\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   } catch (const std::exception& e) {                        \\\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     std::cerr << \"Error: \" << e.what() << std::endl;         \\\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     return AOTI_RUNTIME_FAILURE;                             \\\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   } catch (...) {                                            \\\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     std::cerr << \"Unknown exception occurred.\" << std::endl; \\\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     return AOTI_RUNTIME_FAILURE;                             \\\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   }                                                          \\\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   return AOTI_RUNTIME_SUCCESS;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] #define AOTI_VECTOR_SIZE_CHECK(actual_size, expected_size, name)  \\\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   do {                                                            \\\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTI_RUNTIME_CHECK(                                           \\\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         actual_size == expected_size,                             \\\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         \"expected \" + std::string(name) + \" vector size to be \" + \\\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]             std::to_string(expected_size) + \", but got \" +        \\\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]             std::to_string(actual_size));                         \\\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   } while (0)\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] // AOTInductor uses at::addmm_out, which doesn't supports\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] // arguments that requires gradient. For this reason, we\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] // enforce no_grad context for run APIs.\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] //\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] // A RAII, thread local (!) guard that enables or disables grad mode upon\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] // construction, and sets it back to the original value upon destruction.\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] struct AOTINoGradGuard {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   AOTINoGradGuard() : prev_mode(aoti_torch_grad_mode_is_enabled()) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     aoti_torch_grad_mode_set_enabled(false);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   ~AOTINoGradGuard() {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     aoti_torch_grad_mode_set_enabled(prev_mode);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   bool prev_mode;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] };\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] extern \"C\" {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] AOTIRuntimeError AOTInductorModelContainerCreate(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTInductorModelContainerHandle* container_handle,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     size_t num_models,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     bool is_cpu,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     const char* cubin_dir) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       return AOTInductorModelContainerCreateWithDevice(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         container_handle,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         num_models,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         is_cpu ? \"cpu\" : \"cuda\",\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         cubin_dir);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] AOTIRuntimeError AOTInductorModelContainerCreateWithDevice(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTInductorModelContainerHandle* container_handle,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     size_t num_models,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     const char* device_str,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     const char* cubin_dir) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   if (num_models == 0) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     std::cerr << \"Error: num_models must be positive, but got 0\" << std::endl;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     return AOTI_RUNTIME_FAILURE;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   CONVERT_EXCEPTION_TO_ERROR_CODE({\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     std::optional<std::string> cubin_dir_opt;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     if (cubin_dir != nullptr) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       cubin_dir_opt.emplace(cubin_dir);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     auto* container = new torch::aot_inductor::AOTInductorModelContainer(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         num_models, std::string(device_str), cubin_dir_opt);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     *container_handle =\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         reinterpret_cast<AOTInductorModelContainerHandle>(container);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   })\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] AOTIRuntimeError AOTInductorModelContainerDelete(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTInductorModelContainerHandle container_handle) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   CONVERT_EXCEPTION_TO_ERROR_CODE({\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     auto* container =\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         reinterpret_cast<torch::aot_inductor::AOTInductorModelContainer*>(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]             container_handle);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     delete container;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   });\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] AOTIRuntimeError AOTInductorModelContainerRun(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTInductorModelContainerHandle container_handle,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AtenTensorHandle* input_handles, // array of input AtenTensorHandle; handles\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]                                      // are stolen; the array itself is borrowed\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     size_t num_inputs,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AtenTensorHandle*\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         output_handles, // array for writing output AtenTensorHandle; handles\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]                         // will be stolen by the caller; the array itself is\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]                         // borrowed\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     size_t num_outputs,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTInductorStreamHandle stream_handle,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTIProxyExecutorHandle proxy_executor_handle) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   auto* container =\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       reinterpret_cast<torch::aot_inductor::AOTInductorModelContainer*>(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]           container_handle);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   AOTI_VECTOR_SIZE_CHECK(num_inputs, container->num_inputs(), \"inputs\");\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   AOTI_VECTOR_SIZE_CHECK(num_outputs, container->num_outputs(), \"outputs\");\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   auto stream =\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       reinterpret_cast<torch::aot_inductor::DeviceStreamType>(stream_handle);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   CONVERT_EXCEPTION_TO_ERROR_CODE({\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTINoGradGuard guard;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     container->run(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         input_handles, output_handles, stream, proxy_executor_handle);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   })\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] AOTIRuntimeError AOTInductorModelContainerGetNumConstants(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTInductorModelContainerHandle container_handle,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     size_t* num_constants) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   auto* container =\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       reinterpret_cast<torch::aot_inductor::AOTInductorModelContainer*>(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]           container_handle);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   CONVERT_EXCEPTION_TO_ERROR_CODE(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     { *num_constants = container->num_constants(); })\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] AOTIRuntimeError AOTInductorModelContainerGetConstantName(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTInductorModelContainerHandle container_handle,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     size_t idx,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     const char** name) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   auto* container =\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       reinterpret_cast<torch::aot_inductor::AOTInductorModelContainer*>(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]           container_handle);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   CONVERT_EXCEPTION_TO_ERROR_CODE(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     { *name = container->constant_name(idx); })\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] AOTIRuntimeError AOTInductorModelContainerGetConstantOriginalFQN(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTInductorModelContainerHandle container_handle,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     size_t idx,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     const char** original_fqn) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   auto* container =\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       reinterpret_cast<torch::aot_inductor::AOTInductorModelContainer*>(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]           container_handle);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   CONVERT_EXCEPTION_TO_ERROR_CODE(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     { *original_fqn = container->constant_original_fqn(idx); })\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] AOTIRuntimeError AOTInductorModelContainerGetConstantFromFolded(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTInductorModelContainerHandle container_handle,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     size_t idx,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     bool* from_folded) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   auto* container =\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       reinterpret_cast<torch::aot_inductor::AOTInductorModelContainer*>(container_handle);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   CONVERT_EXCEPTION_TO_ERROR_CODE({ *from_folded = container->constant_from_folded(idx); })\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] AOTIRuntimeError AOTInductorModelContainerGetConstantDtype(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTInductorModelContainerHandle container_handle,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     size_t idx,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     int32_t* dtype) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   auto* container =\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       reinterpret_cast<torch::aot_inductor::AOTInductorModelContainer*>(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]           container_handle);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   CONVERT_EXCEPTION_TO_ERROR_CODE(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     { *dtype = container->constant_dtype(idx); })\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] AOTIRuntimeError AOTInductorModelContainerUpdateConstantBuffer(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTInductorModelContainerHandle container_handle,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTInductorConstantMapHandle constant_map_handle,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     bool use_inactive,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     bool validate_full_update) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   auto* container =\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       reinterpret_cast<torch::aot_inductor::AOTInductorModelContainer*>(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]           container_handle);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   auto input_map = reinterpret_cast<std::unordered_map<std::string, AtenTensorHandle>*>(constant_map_handle);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   CONVERT_EXCEPTION_TO_ERROR_CODE({\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     container->update_constant_buffer(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         *input_map, use_inactive, validate_full_update);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   })\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] AOTIRuntimeError AOTInductorModelContainerUpdateInactiveConstantBuffer(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTInductorModelContainerHandle container_handle,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTInductorConstantMapHandle constant_map_handle) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   return AOTInductorModelContainerUpdateConstantBuffer(container_handle,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]           constant_map_handle,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]           /*use_inactive*/ true,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]           /*validate_full_update*/ true);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] AOTIRuntimeError AOTInductorModelContainerRunConstantFolding(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTInductorModelContainerHandle container_handle,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     bool use_inactive,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTInductorStreamHandle stream_handle,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTIProxyExecutorHandle proxy_executor_handle) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   auto* container =\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       reinterpret_cast<torch::aot_inductor::AOTInductorModelContainer*>(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]           container_handle);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   auto stream =\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       reinterpret_cast<torch::aot_inductor::DeviceStreamType>(stream_handle);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   CONVERT_EXCEPTION_TO_ERROR_CODE({\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTINoGradGuard guard;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     container->run_const_fold(use_inactive, stream, proxy_executor_handle);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   })\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] AOTIRuntimeError AOTInductorModelContainerSwapConstantBuffer(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTInductorModelContainerHandle container_handle) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   auto* container =\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       reinterpret_cast<torch::aot_inductor::AOTInductorModelContainer*>(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]           container_handle);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   CONVERT_EXCEPTION_TO_ERROR_CODE({\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     container->swap_constant_buffer();\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   })\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] AOTIRuntimeError AOTInductorModelContainerGetNumInputs(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTInductorModelContainerHandle container_handle,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     size_t* ret_num_inputs) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   auto* container =\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       reinterpret_cast<torch::aot_inductor::AOTInductorModelContainer*>(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]           container_handle);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   CONVERT_EXCEPTION_TO_ERROR_CODE(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       { *ret_num_inputs = container->num_inputs(); })\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] AOTIRuntimeError AOTInductorModelContainerGetInputName(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTInductorModelContainerHandle container_handle,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     size_t input_idx,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     const char** ret_input_names) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   auto* container =\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       reinterpret_cast<torch::aot_inductor::AOTInductorModelContainer*>(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]           container_handle);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   CONVERT_EXCEPTION_TO_ERROR_CODE(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       { *ret_input_names = container->input_name(input_idx); })\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] AOTIRuntimeError AOTInductorModelContainerGetNumOutputs(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTInductorModelContainerHandle container_handle,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     size_t* ret_num_outputs) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   auto* container =\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       reinterpret_cast<torch::aot_inductor::AOTInductorModelContainer*>(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]           container_handle);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   CONVERT_EXCEPTION_TO_ERROR_CODE(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       { *ret_num_outputs = container->num_outputs(); })\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] AOTIRuntimeError AOTInductorModelContainerGetOutputName(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTInductorModelContainerHandle container_handle,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     size_t output_idx,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     const char** ret_output_names) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   auto* container =\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       reinterpret_cast<torch::aot_inductor::AOTInductorModelContainer*>(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]           container_handle);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   CONVERT_EXCEPTION_TO_ERROR_CODE(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       { *ret_output_names = container->output_name(output_idx); })\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] AOTIRuntimeError AOTInductorModelContainerGetCallSpec(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTInductorModelContainerHandle container_handle,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     const char** in_spec,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     const char** out_spec) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   auto* container =\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       reinterpret_cast<torch::aot_inductor::AOTInductorModelContainer*>(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]           container_handle);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   CONVERT_EXCEPTION_TO_ERROR_CODE({\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     *in_spec = container->get_in_spec();\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     *out_spec = container->get_out_spec();\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   })\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] AOTIRuntimeError AOTInductorModelCreate(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTInductorModelHandle* model_handle,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTInductorConstantMapHandle constant_map_handle){\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     CONVERT_EXCEPTION_TO_ERROR_CODE({\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       auto constant_map = std::make_shared<torch::aot_inductor::ConstantMap>();\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       auto constant_array = std::make_shared<std::vector<torch::aot_inductor::ConstantHandle>>();\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       auto input_map = reinterpret_cast<std::unordered_map<std::string, AtenTensorHandle>*>(constant_map_handle);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       auto model = new torch::aot_inductor::AOTInductorModel(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]           constant_map,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]           constant_array,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]           \"cpu\", // device_str is hardcoded, as AOTInductorModelCreate is only use for CPU models\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]           \"\"\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       );\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       if (input_map) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         for (auto const& kv : *input_map) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]           constant_map->emplace(kv.first, kv.second);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       } else {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         model->load_constants();\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       *model_handle = reinterpret_cast<AOTInductorModelHandle>(model);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     })}\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] AOTIRuntimeError AOTInductorModelRun(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTInductorModelHandle model_handle,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AtenTensorHandle* input_handles,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AtenTensorHandle* output_handles) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   auto model =\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       reinterpret_cast<torch::aot_inductor::AOTInductorModel*>(model_handle);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   CONVERT_EXCEPTION_TO_ERROR_CODE({\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTINoGradGuard guard;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     model->run_impl(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         input_handles,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         output_handles,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         (torch::aot_inductor::DeviceStreamType) nullptr,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         nullptr);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   })\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] AOTIRuntimeError AOTInductorModelDelete(AOTInductorModelHandle model_handle){\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     CONVERT_EXCEPTION_TO_ERROR_CODE({\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       auto model = reinterpret_cast<torch::aot_inductor::AOTInductorModel*>(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]           model_handle);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       delete model;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     })}\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] AOTIRuntimeError AOTInductorModelGetNumOutputs(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTInductorModelHandle model_handle,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     size_t* ret_num_outputs) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   CONVERT_EXCEPTION_TO_ERROR_CODE({\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       auto model = reinterpret_cast<torch::aot_inductor::AOTInductorModel*>(model_handle);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       *ret_num_outputs = model->num_outputs();\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   })\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] AOTIRuntimeError AOTInductorModelUpdateConstantsMap(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTInductorModelHandle model_handle,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTInductorConstantMapHandle constant_map_handle) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   auto model =\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       reinterpret_cast<torch::aot_inductor::AOTInductorModel*>(model_handle);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   CONVERT_EXCEPTION_TO_ERROR_CODE({\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     auto constant_map = std::make_shared<torch::aot_inductor::ConstantMap>();\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     auto input_map =\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         reinterpret_cast<std::unordered_map<std::string, AtenTensorHandle>*>(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]             constant_map_handle);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     for (auto const& kv : *input_map) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       constant_map->emplace(kv.first, kv.second);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     model->update_constants_map(std::move(constant_map));\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   })\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] } // extern \"C\"\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] // NOTE: Like interface.cpp, this file will be copied into AOTInductor\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] // generated output. This file is intended to keep implementation\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] // details separate from the implementation of the AOTI public\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] // interface. Note also that #includes should go into interface.cpp\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] // for simplicity of maintenance.\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] namespace torch {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] namespace aot_inductor {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] template <typename T>\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] void convert_output_to_handle(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     const ArrayRefTensor<T>& output,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AtenTensorHandle& handle) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   handle = output.expensiveCopyToTensor();\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] template <typename... Ts, std::size_t... Is>\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] void convert_outputs_to_handles_helper(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     const std::tuple<ArrayRefTensor<Ts>...>& outputs,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AtenTensorHandle* output_handles,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     std::index_sequence<Is...>) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   (convert_output_to_handle(std::get<Is>(outputs), output_handles[Is]), ...);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] template <typename... Ts>\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] void convert_outputs_to_handles(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     const std::tuple<ArrayRefTensor<Ts>...>& outputs,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AtenTensorHandle* output_handles) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   convert_outputs_to_handles_helper(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       outputs, output_handles, std::make_index_sequence<sizeof...(Ts)>());\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] template <typename T>\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] void convert_handle_to_arrayref_tensor(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AtenTensorHandle handle,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     ArrayRefTensor<T>& input) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   void* data_ptr;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_get_data_ptr(handle, &data_ptr));\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   int64_t dim;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_get_dim(handle, &dim));\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   int64_t numel;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_get_numel(handle, &numel));\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   int64_t* sizes;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_get_sizes(handle, &sizes));\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   int64_t* strides;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_get_strides(handle, &strides));\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   int32_t dtype;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_get_dtype(handle, &dtype));\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   int32_t device_type;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_get_device_type(handle, &device_type));\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   int32_t device_index;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   AOTI_TORCH_ERROR_CODE_CHECK(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       aoti_torch_get_device_index(handle, &device_index));\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   input = ArrayRefTensor<T>(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       MiniArrayRef<T>(reinterpret_cast<T*>(data_ptr), numel),\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       MiniArrayRef<const int64_t>(sizes, dim),\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       MiniArrayRef<const int64_t>(strides, dim),\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       device_type,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       device_index);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] template <typename... Ts, std::size_t... Is>\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] void convert_handles_to_inputs_helper(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AtenTensorHandle* input_handles,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     std::tuple<ArrayRefTensor<Ts>...>& inputs,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     std::index_sequence<Is...>) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   (convert_handle_to_arrayref_tensor(input_handles[Is], std::get<Is>(inputs)),\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]    ...);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] template <typename... Ts>\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] void convert_handles_to_inputs(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AtenTensorHandle* input_handles,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     std::tuple<ArrayRefTensor<Ts>...>& inputs) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   convert_handles_to_inputs_helper(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]       input_handles, inputs, std::make_index_sequence<sizeof...(Ts)>());\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] template <typename T>\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] void assert_numel(const ArrayRefTensor<T>& tensor, int64_t numel) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   if (tensor.numel() != numel) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     std::stringstream err;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     err << \"incorrect numel for input tensor. expected \" << numel << \", got \" << tensor.numel();\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     throw std::runtime_error(err.str());\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] } // namespace aot_inductor\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] } // namespace torch\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] #include <ATen/ATen.h>\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] #include <ATen/core/dispatch/Dispatcher.h>\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] #include <ATen/native/BinaryOps.h>\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] #include <torch/csrc/inductor/aoti_runtime/utils.h>\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] #include <torch/csrc/inductor/aoti_torch/tensor_converter.h>\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] #include <torch/csrc/inductor/aoti_torch/utils.h>\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] #include <torch/csrc/inductor/inductor_ops.h>\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] #include <torch/types.h>\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] #include <ATen/ops/bernoulli_native.h>\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] #define reinterpret_tensor torch::inductor::_reinterpret_tensor\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] #define alloc_from_pool torch::inductor::_alloc_from_pool\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] #include <c10/util/generic_math.h>\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] [[maybe_unused]] static int64_t align(int64_t nbytes) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   return (nbytes + 64 - 1) & -64;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] #include \"/tmp/torchinductor_root/ub/cub6x5nmhqhp7xapkb3dlgjxef3t2bnkx7y7n4z2f4z5obnecxpy.h\"\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] extern \"C\" void cpp_fused_relu_0(float* in_out_ptr0,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]                        float* out_ptr0,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]                        const long ks0)\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         for(long x0=static_cast<long>(0L); x0<static_cast<long>(16L*ks0); x0+=static_cast<long>(8L))\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]             auto tmp0 = at::vec::Vectorized<float>::loadu(in_out_ptr0 + static_cast<long>(x0), 8);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]             auto tmp1 = at::vec::clamp_min(tmp0, decltype(tmp0)(0));\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]             tmp1.store(in_out_ptr0 + static_cast<long>(x0));\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         auto tmp0 = static_cast<float>(0.1357109248638153);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         out_ptr0[static_cast<long>(0L)] = tmp0;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] #include \"/tmp/torchinductor_root/ub/cub6x5nmhqhp7xapkb3dlgjxef3t2bnkx7y7n4z2f4z5obnecxpy.h\"\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] extern \"C\" void cpp_fused_sigmoid_1(float* in_out_ptr0,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]                        const long ks0)\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         for(long x0=static_cast<long>(0L); x0<static_cast<long>(8L*(c10::div_floor_integer(ks0, 8L))); x0+=static_cast<long>(8L))\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]             auto tmp0 = at::vec::Vectorized<float>::loadu(in_out_ptr0 + static_cast<long>(x0), 8);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]             auto tmp1 = decltype(tmp0)(1)/(decltype(tmp0)(1) + tmp0.neg().exp());\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]             tmp1.store(in_out_ptr0 + static_cast<long>(x0));\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         #pragma omp simd simdlen(4) \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         for(long x0=static_cast<long>(8L*(c10::div_floor_integer(ks0, 8L))); x0<static_cast<long>(ks0); x0+=static_cast<long>(1L))\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]             auto tmp0 = in_out_ptr0[static_cast<long>(x0)];\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]             auto tmp1 = decltype(tmp0)(1) / (decltype(tmp0)(1) + std::exp(-tmp0));\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]             in_out_ptr0[static_cast<long>(x0)] = tmp1;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] namespace torch {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] namespace aot_inductor {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] namespace {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] class AOTInductorModelKernels : public AOTInductorModelKernelsBase {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]   public:\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] };\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }  // namespace\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] AOTInductorModel::AOTInductorModel(std::shared_ptr<ConstantMap> constants_map,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]                                    std::shared_ptr<std::vector<ConstantHandle>> constants_array,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]                                    const std::string& device_str,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]                                    std::optional<std::string> cubin_dir)\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     : AOTInductorModelBase(1, 1, 3, device_str, cubin_dir) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     inputs_info_[0].name = \"arg4_1\";\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     constants_info_[0].name = \"L__self___fc1_weight\";\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     constants_info_[0].dtype = static_cast<int32_t>(at::kFloat);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     constants_info_[0].offset = 0;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     constants_info_[0].data_size = 640;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     constants_info_[0].from_folded = false;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     constants_info_[0].shape = {16, 10};\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     constants_info_[0].stride = {10, 1};\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     constants_info_[0].original_fqn = \"fc1.weight\";\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     constants_info_[1].name = \"L__self___fc1_bias\";\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     constants_info_[1].dtype = static_cast<int32_t>(at::kFloat);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     constants_info_[1].offset = 0;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     constants_info_[1].data_size = 64;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     constants_info_[1].from_folded = false;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     constants_info_[1].shape = {16};\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     constants_info_[1].stride = {1};\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     constants_info_[1].original_fqn = \"fc1.bias\";\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     constants_info_[2].name = \"L__self___fc2_weight\";\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     constants_info_[2].dtype = static_cast<int32_t>(at::kFloat);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     constants_info_[2].offset = 0;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     constants_info_[2].data_size = 64;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     constants_info_[2].from_folded = false;\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     constants_info_[2].shape = {1, 16};\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     constants_info_[2].stride = {16, 1};\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     constants_info_[2].original_fqn = \"fc2.weight\";\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     update_constants_map(std::move(constants_map));\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     update_constants_array(std::move(constants_array));\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     in_spec_ = \"[1, {\\\"type\\\": \\\"builtins.tuple\\\", \\\"context\\\": \\\"null\\\", \\\"children_spec\\\": [{\\\"type\\\": \\\"builtins.tuple\\\", \\\"context\\\": \\\"null\\\", \\\"children_spec\\\": [{\\\"type\\\": null, \\\"context\\\": null, \\\"children_spec\\\": []}]}, {\\\"type\\\": \\\"builtins.dict\\\", \\\"context\\\": \\\"[]\\\", \\\"children_spec\\\": []}]}]\";\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     out_spec_ = \"[1, {\\\"type\\\": null, \\\"context\\\": null, \\\"children_spec\\\": []}]\";\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     outputs_info_[0].name = \"output0\";\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     this->kernels_ = std::make_unique<AOTInductorModelKernels>();\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] std::unordered_map<std::string, AtenTensorHandle> AOTInductorModel::const_run_impl(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     DeviceStreamType stream,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTIProxyExecutorHandle proxy_executor,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     bool initialization\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] ) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     if (!initialization) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         std::cerr << \"[WARNING] Calling constant_folding in model, but compiled with config: \"\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]                   << \"aot_inductor.use_runtime_constant_folding=False\\n\";\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     return {};\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] }\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] void AOTInductorModel::_const_run_impl(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     std::vector<AtenTensorHandle>& output_handles,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     DeviceStreamType stream,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTIProxyExecutorHandle proxy_executor\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] ) {}\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] void AOTInductorModel::run_impl(\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AtenTensorHandle*\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         input_handles, // array of input AtenTensorHandle; handles\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]                         // are stolen; the array itself is borrowed\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AtenTensorHandle*\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]         output_handles, // array for writing output AtenTensorHandle; handles\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]                         // will be stolen by the caller; the array itself is\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]                         // borrowed\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     DeviceStreamType stream,\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     AOTIProxyExecutorHandle proxy_executor\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] ) {\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     auto inputs = alloc_tensors_by_stealing_from_handles(input_handles, 1);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     auto arg4_1 = std::move(inputs[0]);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     auto L__self___fc1_weight = *tensor_handle_to_tensor_pointer(constants_->at(0));\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     auto L__self___fc1_bias = *tensor_handle_to_tensor_pointer(constants_->at(1));\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     auto L__self___fc2_weight = *tensor_handle_to_tensor_pointer(constants_->at(2));\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     auto arg4_1_size = arg4_1.sizes();\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     auto s0 = arg4_1_size[0];\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     inputs.clear();\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     auto& kernels = static_cast<AOTInductorModelKernels&>(*this->kernels_.get());\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     at::Tensor buf0 = at::detail::empty_strided_cpu({s0, 16L}, {16L, 1L}, at::kFloat);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     // Source Nodes: [x], Original ATen: [aten.addmm]\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     at::addmm_out(buf0, L__self___fc1_bias, arg4_1, reinterpret_tensor(L__self___fc1_weight, {10L, 16L}, {1L, 10L}, 0L), 1L, 1L);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     arg4_1.reset();\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     decltype(auto) buf1 = buf0; buf0.reset();;  // reuse\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     at::Tensor buf2 = at::detail::empty_strided_cpu({1L, }, {1L, }, at::kFloat);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     cpp_fused_relu_0((float*)(buf1.data_ptr()), (float*)(buf2.data_ptr()), s0);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     at::Tensor buf4 = at::detail::empty_strided_cpu({s0, 1L}, {1L, 1L}, at::kFloat);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     // Source Nodes: [x_1, x_2], Original ATen: [aten.addmm, aten.relu]\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     at::addmm_out(buf4, buf2, buf1, reinterpret_tensor(L__self___fc2_weight, {16L, 1L}, {1L, 16L}, 0L), 1L, 1L);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     buf1.reset();\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     buf2.reset();\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     decltype(auto) buf5 = buf4; buf4.reset();;  // reuse\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     cpp_fused_sigmoid_1((float*)(buf5.data_ptr()), s0);\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code]     output_handles[0] = reinterpret_cast<AtenTensorHandle>(new at::Tensor(buf5));\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] } // AOTInductorModel::run_impl\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] } // namespace aot_inductor\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] } // namespace torch\n",
            "V0426 21:16:11.469000 139836882890752 torch/_inductor/graph.py:1601] [__output_code] \n",
            "I0426 21:16:11.475000 139836882890752 torch/_inductor/codecache.py:1765] [__output_code] Output code written to: /content/c5xj2musgrt7pxq75w66ed6oyz3s3onvsjm6fjy4arpejy2tldhi.cpp\n"
          ]
        }
      ],
      "source": [
        "torch._logging.set_logs(output_code=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    model = Model().to(device=device)\n",
        "    example_inputs=(torch.randn(8, 10, device=device),)\n",
        "    batch_dim = torch.export.Dim(\"batch\", min=1, max=1024)\n",
        "    so_path = torch._export.aot_compile(\n",
        "        model,\n",
        "        example_inputs,\n",
        "        # Specify the first dimension of the input x as dynamic\n",
        "        dynamic_shapes={\"x\": {0: batch_dim}},\n",
        "        # Specify the generated shared library path\n",
        "        options={\"aot_inductor.output_path\": os.path.join(os.getcwd(), \"model.so\")},\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}