{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"suwXRfQ8SsGL","executionInfo":{"status":"ok","timestamp":1677761024014,"user_tz":-480,"elapsed":2348,"user":{"displayName":"Meng Wang","userId":"16239936616224806188"}}},"outputs":[],"source":["\"\"\"Module providingFunction printing python version.\"\"\"\n","import torch.nn as nn\n","import torch\n","import torch._inductor.config\n","\n","torch._inductor.config.debug = True\n","torch._inductor.config.trace.output_code = True\n","torch._inductor.config.verbose_progress = True\n","torch._inductor.config.trace.graph_diagram = True\n","torch._inductor.config.cpp.cxx = (\"g++\",)"]},{"cell_type":"code","source":["!pip3 install --pre torch==2.0.0.dev20230301 --index-url https://download.pytorch.org/whl/nightly/cpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"59zF-VK1SuIS","executionInfo":{"status":"ok","timestamp":1677761021670,"user_tz":-480,"elapsed":46817,"user":{"displayName":"Meng Wang","userId":"16239936616224806188"}},"outputId":"a39b392f-e41a-49e3-99f0-85f67969e9ce"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://download.pytorch.org/whl/nightly/cpu, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch==2.0.0.dev20230301\n","  Downloading https://download.pytorch.org/whl/nightly/cpu/torch-2.0.0.dev20230301%2Bcpu-cp38-cp38-linux_x86_64.whl (196.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch==2.0.0.dev20230301) (3.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch==2.0.0.dev20230301) (1.7.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==2.0.0.dev20230301) (4.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch==2.0.0.dev20230301) (3.9.0)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch==2.0.0.dev20230301) (1.2.1)\n","Installing collected packages: torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.13.1+cu116\n","    Uninstalling torch-1.13.1+cu116:\n","      Successfully uninstalled torch-1.13.1+cu116\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.14.1+cu116 requires torch==1.13.1, but you have torch 2.0.0.dev20230301+cpu which is incompatible.\n","torchtext 0.14.1 requires torch==1.13.1, but you have torch 2.0.0.dev20230301+cpu which is incompatible.\n","torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 2.0.0.dev20230301+cpu which is incompatible.\n","fastai 2.7.11 requires torch<1.14,>=1.7, but you have torch 2.0.0.dev20230301+cpu which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed torch-2.0.0.dev20230301+cpu\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tf743HgKSsGO"},"outputs":[],"source":["\"\"\"Module providingFunction printing python version.\"\"\"\n","class MLP(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.fc1 = nn.Linear(32, 64)\n","        \n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = torch.nn.functional.gelu(x)\n","        return x\n","\n","model = MLP()\n","\n","batch_size = 8\n","input = torch.randn(batch_size, 32)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EEChoCYISsGO","executionInfo":{"status":"ok","timestamp":1676562992251,"user_tz":-480,"elapsed":347,"user":{"displayName":"Meng Wang","userId":"16239936616224806188"}},"outputId":"1c65a07f-66a2-4684-b096-0d076eee3f1d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dynamo produced a fx graph in Torch IR:\n","class GraphModule(torch.nn.Module):\n","    def forward(self, x : torch.Tensor):\n","        # File: <ipython-input-3-bcf5f62b34e9>:8, code: x = self.fc1(x)\n","        self_fc1 = self.self_fc1(x);  x = None\n","        \n","        # File: <ipython-input-3-bcf5f62b34e9>:9, code: x = torch.nn.functional.gelu(x)\n","        gelu = torch._C._nn.gelu(self_fc1);  self_fc1 = None\n","        return (gelu,)\n","        \n","Notice that sample_input is a list of flattened FakeTensor:\n","[FakeTensor(FakeTensor(..., device='meta', size=(8, 32)), cpu)]\n"]}],"source":["import torch._dynamo\n","from torch._functorch.aot_autograd import aot_module_simplified\n","\n","def toy_backend(gm, sample_input):\n","    print('Dynamo produced a fx graph in Torch IR:')\n","    gm.print_readable()\n","    \n","    print('Notice that sample_input is a list of flattened FakeTensor:')\n","    print(sample_input)\n","    return gm.forward\n","\n","torch._dynamo.reset()\n","fn = torch.compile(backend=toy_backend)(model)\n","\n","out = fn(input)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JFzgKMcNSsGP","executionInfo":{"status":"ok","timestamp":1676563006228,"user_tz":-480,"elapsed":748,"user":{"displayName":"Meng Wang","userId":"16239936616224806188"}},"outputId":"5fe64e9f-7c2e-41fc-9a72-49bc73622acc"},"outputs":[{"output_type":"stream","name":"stdout","text":["AOTAutograd produced a fx graph in Aten IR:\n","class GraphModule(torch.nn.Module):\n","    def forward(self, primals_1: f32[64, 32], primals_2: f32[64], primals_3: f32[s0, 32]):\n","        # File: <ipython-input-3-bcf5f62b34e9>:8, code: x = self.fc1(x)\n","        t: f32[32, 64] = torch.ops.aten.t.default(primals_1);  primals_1 = None\n","        addmm: f32[s0, 64] = torch.ops.aten.addmm.default(primals_2, primals_3, t);  primals_2 = t = None\n","        \n","        # File: <ipython-input-3-bcf5f62b34e9>:9, code: x = torch.nn.functional.gelu(x)\n","        gelu: f32[s0, 64] = torch.ops.aten.gelu.default(addmm)\n","        return [gelu, primals_3, addmm]\n","        \n","AOTAutograd produced a fx graph in Aten IR:\n","class GraphModule(torch.nn.Module):\n","    def forward(self, primals_3: f32[s0, 32], addmm: f32[s0, 64], tangents_1: f32[s0, 64]):\n","        # File: <ipython-input-3-bcf5f62b34e9>:9, code: x = torch.nn.functional.gelu(x)\n","        gelu_backward: f32[s0, 64] = torch.ops.aten.gelu_backward.default(tangents_1, addmm);  tangents_1 = addmm = None\n","        \n","        # File: <ipython-input-3-bcf5f62b34e9>:8, code: x = self.fc1(x)\n","        t_1: f32[64, s0] = torch.ops.aten.t.default(gelu_backward)\n","        mm: f32[64, 32] = torch.ops.aten.mm.default(t_1, primals_3);  t_1 = primals_3 = None\n","        t_2: f32[32, 64] = torch.ops.aten.t.default(mm);  mm = None\n","        sum_1: f32[1, 64] = torch.ops.aten.sum.dim_IntList(gelu_backward, [0], True);  gelu_backward = None\n","        view: f32[64] = torch.ops.aten.view.default(sum_1, [64]);  sum_1 = None\n","        t_3: f32[64, 32] = torch.ops.aten.t.default(t_2);  t_2 = None\n","        return [t_3, view, None]\n","        \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py:1251: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n","  warnings.warn(\n"]}],"source":["import torch._dynamo\n","from torch._functorch.aot_autograd import aot_module_simplified\n","\n","def toy_backend(gm, sample_input):\n","    def my_compiler(gm, sample_input):\n","        print('AOTAutograd produced a fx graph in Aten IR:')\n","        gm.print_readable()\n","        return gm.forward\n","    # Invoke AOTAutograd\n","    return aot_module_simplified(\n","        gm,\n","        sample_input,\n","        fw_compiler=my_compiler\n","    )\n","\n","torch._dynamo.reset()\n","fn = torch.compile(backend=toy_backend, dynamic=True)(model)\n","out = fn(input)\n","out.sum().backward()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QgtkpRubSsGQ"},"outputs":[],"source":["from torch._inductor.decomposition import decompositions as default_decompositions\n","\n","decompositions = default_decompositions.copy()\n","default_decompositions.update(\n","    torch._decomp.get_decompositions([\n","        torch.ops.aten.addmm,\n","    ])\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KLtn0Xy4SsGQ"},"outputs":[],"source":["def toy_backend(gm, sample_input):\n","    def my_compiler(gm, sample_input):\n","        print('Decomposed fx graph in Aten IR:')\n","        gm.print_readable()\n","        return gm\n","    # Invoke AOTAutograd\n","    return aot_module_simplified(\n","        gm,\n","        sample_input,\n","        decompositions=decompositions,\n","        fw_compiler=my_compiler\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3WA8kEqXSsGR","executionInfo":{"status":"ok","timestamp":1676563051412,"user_tz":-480,"elapsed":879,"user":{"displayName":"Meng Wang","userId":"16239936616224806188"}},"outputId":"3a5dce57-9df5-4f90-98ea-506588a43497"},"outputs":[{"output_type":"stream","name":"stdout","text":["Decomposed fx graph in Aten IR:\n","class GraphModule(torch.nn.Module):\n","    def forward(self, primals_1: f32[64, 32], primals_2: f32[64], primals_3: f32[s0, 32]):\n","        # File: <ipython-input-3-bcf5f62b34e9>:8, code: x = self.fc1(x)\n","        permute: f32[32, 64] = torch.ops.aten.permute.default(primals_1, [1, 0]);  primals_1 = None\n","        addmm: f32[s0, 64] = torch.ops.aten.addmm.default(primals_2, primals_3, permute);  primals_2 = permute = None\n","        \n","        # File: <ipython-input-3-bcf5f62b34e9>:9, code: x = torch.nn.functional.gelu(x)\n","        mul: f32[s0, 64] = torch.ops.aten.mul.Tensor(addmm, 0.5)\n","        mul_1: f32[s0, 64] = torch.ops.aten.mul.Tensor(addmm, 0.7071067811865476)\n","        erf: f32[s0, 64] = torch.ops.aten.erf.default(mul_1);  mul_1 = None\n","        add: f32[s0, 64] = torch.ops.aten.add.Tensor(erf, 1);  erf = None\n","        mul_2: f32[s0, 64] = torch.ops.aten.mul.Tensor(mul, add);  mul = add = None\n","        return [mul_2, primals_3, addmm]\n","        \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py:1251: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n","  warnings.warn(\n"]}],"source":["torch._dynamo.reset()\n","fn = torch.compile(backend=toy_backend, dynamic=True)(model)\n","out = fn(input)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u_gx7QsCSsGR","executionInfo":{"status":"ok","timestamp":1676563057940,"user_tz":-480,"elapsed":418,"user":{"displayName":"Meng Wang","userId":"16239936616224806188"}},"outputId":"85ddc41c-01bc-4487-b372-89f9adb5e3d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Further Decomposed fx graph in Aten IR:\n","class <lambda>(torch.nn.Module):\n","    def forward(self, arg0_1: f32[3], arg1_1: f16[3, 3]):\n","        # File: <ipython-input-9-c65886766946>:7, code: return a + b\n","        _to_copy: f32[3, 3] = torch.ops.aten._to_copy.default(arg1_1, dtype = torch.float32);  arg1_1 = None\n","        broadcast_in_dim: f32[3, 3] = torch.ops.prims.broadcast_in_dim.default(arg0_1, [3, 3], [1]);  arg0_1 = None\n","        add: f32[3, 3] = torch.ops.prims.add.default(broadcast_in_dim, _to_copy);  broadcast_in_dim = _to_copy = None\n","        return (add,)\n","        \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py:1251: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n","  warnings.warn(\n"]}],"source":["prims_decomp = torch._decomp.get_decompositions([\n","    torch.ops.aten.add,\n","    torch.ops.aten.expand.default,\n","])\n","\n","def fn(a, b):\n","    return a + b\n","\n","def toy_backend(gm, sample_input):\n","    def my_compiler(gm, sample_input):\n","        print('Further Decomposed fx graph in Aten IR:')\n","        gm.print_readable()\n","        return gm\n","    # Invoke AOTAutograd\n","    return aot_module_simplified(\n","        gm,\n","        sample_input,\n","        decompositions=prims_decomp,\n","        fw_compiler=my_compiler\n","    )\n","    \n","torch._dynamo.reset()\n","fn = torch.compile(backend=toy_backend)(fn)\n","out = fn(torch.randn(3, dtype=torch.float), torch.randn(3, 3, dtype=torch.half))"]}],"metadata":{"kernelspec":{"display_name":"torch-mlir","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"17acc1b4823e2d7760d359d97f70ebdeaaee0b4115be476c35cc4e7c6900a15c"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}